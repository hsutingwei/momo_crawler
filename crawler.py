# -*- coding: utf-8 -*-
import requests
import json
import pandas as pd
import time
from seleniumwire import webdriver # éœ€å®‰è£ï¼špip install selenium-wire
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
import re
import random
import zlib
from tqdm import tqdm
from bs4 import BeautifulSoup
import os
from datetime import datetime
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

keyword = 'ç¶­ä»–å‘½'
page = 60
ecode = 'utf-8-sig'
product_csv_path = f'{keyword}_å•†å“è³‡æ–™.csv'
snapshot_path = f'crawler/{keyword}_å•†å“éŠ·å”®å¿«ç…§.csv'
current_time_str = datetime.now().strftime('%Y%m%d%H%M%S')
# æ˜¯å¦åŒ…å«ç¬¬ä¸€æ¬¡ã€ŒèˆŠè³‡æ–™ã€ä¸­çš„éŠ·å”®é‡ç•¶æˆä¸€ç­†å¿«ç…§è¨˜éŒ„
include_original_snapshot = True

def is_valid_row(line):
    # åˆ¤æ–·è¡Œæ˜¯å¦ç‚ºåˆæ³•çš„é–‹é ­ï¼ˆä¾‹å¦‚ï¼šå•†å“ID é–‹é ­æ˜¯æ•¸å­—ï¼‰
    return re.match(r'^\d+,', line.strip()) is not None

def clean_broken_csv(input_path, output_path):
    with open(input_path, 'r', encoding='utf-8-sig') as infile, open(output_path, 'w', encoding='utf-8-sig') as outfile:
        buffer = ''
        for line in infile:
            line = line.rstrip('\n')
            if is_valid_row(line):
                # è‹¥ä¸Šä¸€è¡Œæœ‰ç´¯ç©ï¼Œå¯«å…¥
                if buffer:
                    outfile.write(buffer + '\n')
                buffer = line
            else:
                buffer += ' ' + line  # åˆä½µç‚ºåŒä¸€è¡Œï¼ˆä¸­é–“åŠ ç©ºæ ¼é¿å…ç›´æ¥é»ä½ï¼‰
        
        # æœ€å¾Œä¸€è¡Œè£œå¯«
        if buffer:
            outfile.write(buffer + '\n')

def extract_number(input_string):
    """
    å¾å­—ä¸²ä¸­æå–æ•¸å­—ï¼Œæ”¯æŒåƒåˆ†ä½ç¬¦è™Ÿï¼Œä¸¦å°‡çµæœè½‰æ›ç‚º int å‹åˆ¥ã€‚

    :param input_string: å«æœ‰æ•¸å­—çš„å­—ä¸²
    :return: æ•´æ•¸å‹åˆ¥çš„æ•¸å­—ï¼Œè‹¥ç„¡æ•¸å­—å‰‡å›å‚³ None
    """
    # å®šç¾©æ­£è¦è¡¨é”å¼ï¼ŒåŒ¹é…å«æœ‰åƒåˆ†ä½çš„æ•¸å­—
    match = re.search(r'\d{1,3}(?:,\d{3})*(?:\.\d+)?', input_string)
    if match:
        # å»æ‰åƒåˆ†ä½ç¬¦è™Ÿï¼Œä¸¦è½‰æ›ç‚º int
        number_str = match.group(0).replace(',', '')
        return int(float(number_str))  # æ”¯æ´æ•´æ•¸å’Œå°æ•¸
    return None
    
   
def get_goods_comments(goods_code, cur_page=1, cust_no="", filter_type="total", host="web", multi_filter_type=None):
    """
    å‘¼å« momoshop API ç²å–å•†å“è©•è«–åˆ—è¡¨

    :param goods_code: å•†å“ä»£ç¢¼
    :param cur_page: ç•¶å‰é é¢ (é è¨­ç‚º 1)
    :param cust_no: ä½¿ç”¨è€…ä»£ç¢¼ (é è¨­ç‚ºç©ºå­—ä¸²)
    :param filter_type: ç¯©é¸é¡å‹ (é è¨­ç‚º 'total')
    :param host: è«‹æ±‚ä¾†æº (é è¨­ç‚º 'web')
    :param multi_filter_type: å¤šé‡ç¯©é¸é¡å‹ (é è¨­ç‚º ['hasComment'])
    :return: å›å‚³ JSON æ ¼å¼çš„è©•è«–è³‡æ–™
    """
    if multi_filter_type is None:
        multi_filter_type = ["hasComment"]

    url = "https://eccapi.momoshop.com.tw/user/getGoodsCommentList"
    headers = {
        "Content-Type": "application/json",  # è¨­å®šç‚º JSON æ ¼å¼
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }
    payload = {
        "curPage": cur_page,
        "custNo": cust_no,
        "filterType": filter_type,
        "goodsCode": str(goods_code),
        "host": host,
        "multiFilterType": multi_filter_type
    }

    try:
        # ç™¼é€ POST è«‹æ±‚
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()  # è‹¥ HTTP ç‹€æ…‹ç¢¼ç‚º 4xx æˆ– 5xxï¼Œå‰‡æ‹‹å‡ºç•°å¸¸

        # è§£æå›æ‡‰ JSON è³‡æ–™
        data = response.json()
        return data
    except requests.RequestException as e:
        print(f"HTTP è«‹æ±‚éŒ¯èª¤: {e}")
        return None
    except json.JSONDecodeError:
        print("ç„¡æ³•è§£æå›æ‡‰çš„ JSON è³‡æ–™")
        return None

def get_current_sales(goods_code, host="momoshop"):
    url = "https://eccapi.momoshop.com.tw/user/getGoodsComment"
    headers = {
        "Content-Type": "application/json",  # è¨­å®šç‚º JSON æ ¼å¼
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }
    payload = {
        "goodsCode": str(goods_code),
        "host": host,
    }

    try:
        # ç™¼é€ POST è«‹æ±‚
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()  # è‹¥ HTTP ç‹€æ…‹ç¢¼ç‚º 4xx æˆ– 5xxï¼Œå‰‡æ‹‹å‡ºç•°å¸¸

        # è§£æå›æ‡‰ JSON è³‡æ–™
        data = response.json()
        if data is None or data.get("saleCount") is None:
            print(f"ã€è­¦å‘Šã€‘ç„¡æ³•å–å¾— {goods_code} çš„éŠ·å”®æ•¸")
            return 0
        sales_text = data.get("saleCount")
        count = extract_number(sales_text)
        if count is None:
            return 0
        return str(count) + ('è¬' if sales_text.endswith('è¬') else '')
    except requests.RequestException as e:
        print(f"HTTP è«‹æ±‚éŒ¯èª¤: {e}")
        return None
    except json.JSONDecodeError:
        print("ç„¡æ³•è§£æå›æ‡‰çš„ JSON è³‡æ–™")
        return None

def save_sales_snapshot_long_format():
    if not os.path.exists(product_csv_path):
        print(f"æ‰¾ä¸åˆ°å•†å“è³‡æ–™æª”æ¡ˆï¼š{product_csv_path}")
        return

    df = pd.read_csv(product_csv_path, encoding=ecode)
    all_rows = []

    # âœ… åŠ å…¥ç¬¬ä¸€æ¬¡åŸå§‹è³‡æ–™çš„éŠ·å”®å¿«ç…§
    if include_original_snapshot:
        for _, row in df.iterrows():
            all_rows.append([
                row['å•†å“ID'],
                row['å•†å“åç¨±'],
                row['åƒ¹æ ¼'],
                row['éŠ·å”®æ•¸é‡'],
                row['å•†å“é€£çµ'],
                current_time_str
            ])
        print(f'å·²å¾åŸå§‹å•†å“è³‡æ–™åŠ å…¥ {len(df)} ç­†åˆå§‹å¿«ç…§')

    # âœ… çˆ¬å–ç•¶ä¸‹éŠ·å”®é‡
    service = ChromeService(ChromeDriverManager().install())
    options = webdriver.ChromeOptions()
    options.add_experimental_option("prefs", {"profile.default_content_setting_values.notifications": 2})
    driver = webdriver.Chrome(service=service, options=options)

    for _, row in df.iterrows():
        product_id = row['å•†å“ID']
        product_name = row['å•†å“åç¨±']
        price = row['åƒ¹æ ¼']
        link = row['å•†å“é€£çµ']

        latest_sales = get_current_sales(driver, link)
        if latest_sales is None:
            print(f"ã€è­¦å‘Šã€‘ç„¡æ³•å–å¾— {product_name} çš„éŠ·å”®æ•¸")
            latest_sales = 0

        all_rows.append([
            product_id,
            product_name,
            price,
            latest_sales,
            link,
            current_time_str
        ])
        time.sleep(random.uniform(1, 2.2))

    driver.quit()

    if not all_rows:
        print("âš  ç„¡å¯å¯«å…¥çš„å¿«ç…§è³‡æ–™")
        return

    # æº–å‚™æ–°å¿«ç…§è³‡æ–™è¡¨
    df_snapshot = pd.DataFrame(all_rows, columns=[
        'å•†å“ID', 'å•†å“åç¨±', 'åƒ¹æ ¼', 'éŠ·å”®æ•¸é‡', 'å•†å“é€£çµ', 'æ“·å–æ™‚é–“'
    ])

    os.makedirs('crawler', exist_ok=True)

    # âœ… è‹¥å·²æœ‰å¿«ç…§æª”æ¡ˆï¼Œå…ˆè®€å‡ºæ¯”å°ï¼Œé¿å…é‡è¤‡å¯«å…¥
    if os.path.exists(snapshot_path):
        try:
            df_existing = pd.read_csv(snapshot_path, encoding=ecode, dtype={'å•†å“ID': str})
            # è½‰å‹å¾Œé€²è¡Œå»é‡ï¼ˆæ ¹æ“šå•†å“ID+æ“·å–æ™‚é–“ï¼‰
            before_len = len(df_snapshot)
            df_combined = pd.concat([df_existing, df_snapshot], ignore_index=True)
            df_combined.drop_duplicates(subset=['å•†å“ID', 'æ“·å–æ™‚é–“'], keep='first', inplace=True)
            new_records = df_combined[~df_combined.duplicated(subset=['å•†å“ID', 'æ“·å–æ™‚é–“'], keep='last')]

            df_snapshot = new_records[df_snapshot.columns]
            actual_new = len(df_snapshot)
            if actual_new == 0:
                print("ğŸš« æ²’æœ‰æ–°å¢çš„å¿«ç…§è³‡æ–™ï¼Œè·³éå¯«å…¥")
                return
            else:
                print(f"âœ… å¯¦éš›å¯«å…¥ {actual_new} ç­†å»é‡å¾Œçš„æ–°å¿«ç…§è³‡æ–™")
                df_snapshot.to_csv(snapshot_path, mode='a', encoding=ecode, index=False, header=False)
        except Exception as e:
            print(f"âŒ è®€å–æˆ–è™•ç†ç¾æœ‰å¿«ç…§æª”æ¡ˆæ™‚å‡ºéŒ¯ï¼š{e}")
    else:
        # ç¬¬ä¸€æ¬¡å»ºç«‹å¿«ç…§æª”
        df_snapshot.to_csv(snapshot_path, encoding=ecode, index=False)
        print(f"âœ… é¦–æ¬¡å»ºç«‹å¿«ç…§æª”ï¼Œå¯«å…¥ {len(df_snapshot)} ç­†")

# è‡ªå‹•ä¸‹è¼‰ChromeDriver
service = ChromeService(executable_path=ChromeDriverManager().install())

# é—œé–‰é€šçŸ¥æé†’
options = webdriver.ChromeOptions()
prefs = {"profile.default_content_setting_values.notifications" : 2}
options.add_experimental_option("prefs",prefs)
# ä¸è¼‰å…¥åœ–ç‰‡ï¼Œæå‡çˆ¬èŸ²é€Ÿåº¦
# options.add_argument('blink-settings=imagesEnabled=false') 

# é–‹å•Ÿç€è¦½å™¨
driver = webdriver.Chrome(service=service, chrome_options=options)
time.sleep(random.randint(5,10))

# é–‹å•Ÿç¶²é ï¼Œé€²åˆ°é¦–é 
driver.get('https://www.momoshop.com.tw' )
time.sleep(random.randint(5,10))

#---------- Part 1. ä¸»è¦å…ˆæŠ“ä¸‹å•†å“åç¨±èˆ‡é€£çµï¼Œä¹‹å¾Œå†æ…¢æ…¢è£œä¸Šè©³ç´°è³‡æ–™ ----------
print('---------- é–‹å§‹é€²è¡Œå•†å“çˆ¬èŸ² ----------')
tStart = time.time()#è¨ˆæ™‚é–‹å§‹
# æº–å‚™ç”¨ä¾†å­˜æ”¾è³‡æ–™çš„é™£åˆ—
itemid = []
shopid =[]
name = []
link = []
price = []
sales = []
if (False):
    for i in tqdm(range(int(page))):
    #for i in tqdm(range(1)):
        driver.get('https://www.momoshop.com.tw/search/searchShop.jsp?keyword=' + keyword + '&cateLevel=0&_isFuzzy=0&searchType=1&curPage=' + str(i))
        time.sleep(random.randint(2,4))
        # æ»¾å‹•é é¢
        for scroll in range(6):
            driver.execute_script('window.scrollBy(0,1000)')
            time.sleep(random.randint(2,3))
        
        # å–å¾—å•†å“å…§å®¹
        for block in driver.find_elements(by=By.XPATH, value='//div[contains(@class, "goodsUrl")]'):
            # å°‡æ•´å€‹ç¶²ç«™çš„Htmlé€²è¡Œè§£æ
            soup = BeautifulSoup(block.get_attribute('innerHTML'), "html.parser")

            tname = soup.select_one('.prdName').text.strip()
            if len(tname) <= 0:
                print('æŠ“ä¸åˆ°è³‡æ–™ï¼Œç›´æ¥æ˜¯ç©ºçš„')
                continue # æ²’æŠ“åˆ°é€™å€‹å•†å“å°±åˆ¥çˆ¬äº†  
            tmpSales = soup.select_one('.totalSales').text.strip()
            salseCount = str(extract_number(tmpSales)) + ("è¬" if tmpSales.endswith("è¬") else "")
            #if (salseCount is None or salseCount < 1000):
                #continue # éŠ·å”®é‡å°æ–¼1000å°±è·³é  
            tprice = soup.select_one('.price > b').text.strip()
            tlink = soup.select_one('.goods-img-url').get('href')
            # ä½¿ç”¨æ­£è¦è¡¨é”å¼åŒ¹é… i_code çš„å€¼
            match = re.search(r'i_code=([^&]+)', tlink)

            i_code = ''
            # ç²å–åŒ¹é…åˆ°çš„å€¼
            if match:
                i_code = match.group(1)

            # åˆ°é€™é‚Šç¢ºèªéƒ½æœ‰æŠ“åˆ°è³‡æ–™ï¼Œæ‰å°‡å®ƒå¡å…¥é™£åˆ—ï¼Œå¦å‰‡å¯èƒ½æœƒæœ‰ç¼ºæ¼
            link.append(tlink)
            itemid.append(i_code)
            #shopid.append(theshopid)
            name.append(tname)
            price.append(tprice)
            sales.append(salseCount)


        time.sleep(random.randint(2,4)) # ä¼‘æ¯ä¹…ä¸€é»

    # å…ˆå°‡æ¯é æŠ“åˆ°çš„å•†å“å„²å­˜ä¸‹ä¾†ï¼Œæ–¹ä¾¿å¾ŒçºŒè¿½è¹¤ä¸¦çˆ¬èŸ²
    dic = {
        'å•†å“ID':itemid,
        #'è³£å®¶ID':shopid,
        'å•†å“åç¨±':name,
        'å•†å“é€£çµ':link,
        'åƒ¹æ ¼':price,
        'éŠ·å”®æ•¸é‡': sales,
        'è³‡æ–™å·²å®Œæ•´çˆ¬å–':[ False for x in range(len(itemid)) ] ,
    }
    pd.DataFrame(dic).to_csv(product_csv_path, encoding = ecode, index=False)

    tEnd = time.time()#è¨ˆæ™‚çµæŸ
    totalTime = int(tEnd - tStart)
    minute = totalTime // 60
    second = totalTime % 60
    print('è³‡æ–™å„²å­˜å®Œæˆï¼ŒèŠ±è²»æ™‚é–“ï¼ˆç´„ï¼‰ï¼š ' + str(minute) + ' åˆ† ' + str(second) + 'ç§’')

#print('---------- é–‹å§‹é€²è¡ŒéŠ·å”®æ•¸é‡å¿«ç…§çˆ¬èŸ² ----------')
#save_sales_snapshot_long_format()

#---------- Part 2. è£œä¸Šå•†å“çš„è©³ç´°è³‡æ–™ï¼Œç”±æ–¼å¤šè¨­äº†çˆ¬å–çš„æ¨™è¨˜ï¼Œå› æ­¤çˆ¬éçš„å°±ä¸æœƒå†çˆ¬äº† ----------
print('---------- é–‹å§‹é€²è¡Œç•™è¨€çˆ¬èŸ²ï¼ˆåªæŠ“å–æ–°ç•™è¨€ï¼‰ ----------')
tStart = time.time()

# ç•¶å‰çˆ¬èŸ²æ™‚é–“ä½œç‚ºæª”åç”¨
comments_file_path = f'crawler/{keyword}_å•†å“ç•™è¨€è³‡æ–™_{current_time_str}.csv'

# å˜—è©¦è®€å–éå»å·²çˆ¬ç•™è¨€IDï¼Œé¿å…é‡è¤‡çˆ¬å–
existing_comment_ids = set()
all_existing_files = [f for f in os.listdir('crawler') if f.startswith(keyword + '_å•†å“ç•™è¨€è³‡æ–™_') and f.endswith('.csv')]

for file in all_existing_files:
    original_path = os.path.join('crawler', file)
    fixed_path = original_path.replace('.csv', '_fixed_temp.csv')

    # ä¿®å¾©è©²æª”æ¡ˆçš„æ›è¡ŒéŒ¯èª¤åˆ°æš«å­˜æª”
    clean_broken_csv(original_path, fixed_path)

    try:
        df = pd.read_csv(fixed_path, encoding=ecode)
        existing_comment_ids.update(df['ç•™è¨€ID'].astype(str).tolist())
    except Exception as e:
        print(f"è®€å–ä¿®å¾©å¾Œæª”æ¡ˆ {file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # ç”¨å®Œå°±åˆªé™¤æš«å­˜æª”
    os.remove(fixed_path)

# æ¬„ä½å®¹å™¨ï¼ˆæ–°å¢ è³‡æ–™æ“·å–æ™‚é–“ï¼‰
data2 = [[] for _ in range(20)]  # åŸæœ¬19å€‹æ¬„ä½ + è³‡æ–™æ“·å–æ™‚é–“

getData = pd.read_csv(product_csv_path, encoding=ecode)
all_rows = []

# âœ… åŠ å…¥ç¬¬ä¸€æ¬¡åŸå§‹è³‡æ–™çš„éŠ·å”®å¿«ç…§
if include_original_snapshot:
    for _, row in getData.iterrows():
        all_rows.append([
            row['å•†å“ID'],
            row['å•†å“åç¨±'],
            row['åƒ¹æ ¼'],
            row['éŠ·å”®æ•¸é‡'],
            row['å•†å“é€£çµ'],
            current_time_str
        ])
    print(f'å·²å¾åŸå§‹å•†å“è³‡æ–™åŠ å…¥ {len(df)} ç­†åˆå§‹å¿«ç…§')

for i in tqdm(range(len(getData))):
    if getData.iloc[i]['è³‡æ–™å·²å®Œæ•´çˆ¬å–'] == True:
        continue

    product_id = int(getData.iloc[i]['å•†å“ID'])
    basic_info = [
        product_id,
        getData.iloc[i]['å•†å“åç¨±'],
        getData.iloc[i]['å•†å“é€£çµ'],
        getData.iloc[i]['åƒ¹æ ¼'],
        getData.iloc[i]['éŠ·å”®æ•¸é‡']
    ]

    tmpDetail = get_goods_comments(product_id)
    if tmpDetail is None or tmpDetail.get("goodsCommentList") is None:
        continue

    # æœ‰æµè¨€æ‰æœƒé‡æ–°çˆ¬éŠ·å”®æ•¸é‡çš„å¿«ç…§ (ä¸ç„¶æ¯æ¬¡éƒ½çˆ¬æœƒå¾ˆæ…¢)
    product_id = getData.iloc[i]['å•†å“ID']
    product_name = getData.iloc[i]['å•†å“åç¨±']
    price = getData.iloc[i]['åƒ¹æ ¼']
    link = getData.iloc[i]['å•†å“é€£çµ']

    latest_sales = get_current_sales(product_id)

    all_rows.append([
        product_id,
        product_name,
        price,
        latest_sales,
        link,
        current_time_str
    ])

    all_pages = int(tmpDetail.get("filterList")[0]['count'])
    loopCount = all_pages // 10 + (1 if all_pages % 10 > 0 else 0)

    for page in range(1, loopCount + 1):
        if page > 1:
            tmpDetail = get_goods_comments(product_id, cur_page=page)
            if tmpDetail is None:
                continue
        itemDetail = tmpDetail.get("goodsCommentList")
        if not itemDetail:
            continue

        for obj in itemDetail:
            comment_id = str(obj["commentId"])
            if comment_id in existing_comment_ids:
                continue  # Skip duplicates
            existing_comment_ids.add(comment_id)

            tmp_str = obj["comment"].replace(',', 'ï¼Œ')
            tmp_str = 'ï¼Œ'.join(tmp_str.splitlines())
            tmp_str = re.sub(r'[ï¼Œ]+', 'ï¼Œ', tmp_str)

            data2[0].append(basic_info[0])
            data2[1].append(basic_info[1])
            data2[2].append(basic_info[2])
            data2[3].append(basic_info[3])
            data2[4].append(basic_info[4])
            data2[5].append(tmp_str)
            data2[6].append(comment_id)
            data2[7].append(obj["customName"].replace(',', 'ï¼Œ'))
            data2[8].append(obj["date"])
            data2[9].append(obj["goodsType"].replace(',', 'ï¼Œ'))
            data2[10].append(obj["imageUrlList"])
            data2[11].append(obj["isLike"])
            data2[12].append(obj["isShowLike"])
            data2[13].append(obj["likeCount"])
            data2[14].append(obj["replyContent"].replace(',', 'ï¼Œ'))
            data2[15].append(obj["replyDate"])
            data2[16].append(obj["score"])
            data2[17].append(obj["videoThumbnailImg"])
            data2[18].append(obj["videoUrl"])
            data2[19].append(current_time_str)  # è³‡æ–™æ“·å–æ™‚é–“

        time.sleep(random.randint(2, 4))

# å„²å­˜æ–°ç•™è¨€è³‡æ–™ï¼ˆä»¥æ–°æª”æ¡ˆå„²å­˜ï¼‰
dic = {
    'å•†å“ID': data2[0],
    'å•†å“åç¨±': data2[1],
    'å•†å“é€£çµ': data2[2],
    'åƒ¹æ ¼': data2[3],
    'éŠ·å”®æ•¸é‡': data2[4],
    'ç•™è¨€': data2[5],
    'ç•™è¨€ID': data2[6],
    'ç•™è¨€è€…åç¨±': data2[7],
    'ç•™è¨€æ™‚é–“': data2[8],
    'å•†å“ Type': data2[9],
    'ç•™è¨€ åœ–': data2[10],
    'isLike': data2[11],
    'isShowLike': data2[12],
    'ç•™è¨€ likeCount': data2[13],
    'ç•™è¨€ replyContent': data2[14],
    'ç•™è¨€ replyDate': data2[15],
    'ç•™è¨€æ˜Ÿæ•¸': data2[16],
    'videoThumbnailImg': data2[17],
    'videoUrl': data2[18],
    'è³‡æ–™æ“·å–æ™‚é–“': data2[19]
}

# å„²å­˜åŸå§‹ç•™è¨€æª”
pd.DataFrame(dic).to_csv(comments_file_path, encoding=ecode, index=False)

# å°å‰›å„²å­˜çš„ç•™è¨€æª”æ¡ˆé€²è¡Œæ›è¡Œä¿®å¾©è™•ç†
fixed_file_path = comments_file_path.replace('.csv', '_fixed.csv')
clean_broken_csv(comments_file_path, fixed_file_path)

# è‹¥è¦ç”¨ä¿®æ­£å¾Œç‰ˆæœ¬ç›´æ¥è¦†è“‹åŸå§‹æª”æ¡ˆï¼ˆé¿å…é›™ä»½ï¼‰ï¼Œå–æ¶ˆä¸‹é¢è¨»è§£
os.replace(fixed_file_path, comments_file_path)

# çµæŸè¨ˆæ™‚
tEnd = time.time()
totalTime = int(tEnd - tStart)
minute = totalTime // 60
second = totalTime % 60
print(f'æ–°ç•™è¨€è³‡æ–™å„²å­˜å®Œæˆï¼Œæª”åï¼š{comments_file_path}ï¼Œè€—æ™‚ï¼šç´„ {minute} åˆ† {second} ç§’')


if not all_rows:
    print("âš  ç„¡å¯å¯«å…¥çš„å¿«ç…§è³‡æ–™")
else:
    # æº–å‚™æ–°å¿«ç…§è³‡æ–™è¡¨
    df_snapshot = pd.DataFrame(all_rows, columns=[
        'å•†å“ID', 'å•†å“åç¨±', 'åƒ¹æ ¼', 'éŠ·å”®æ•¸é‡', 'å•†å“é€£çµ', 'æ“·å–æ™‚é–“'
    ])

    os.makedirs('crawler', exist_ok=True)

    # âœ… è‹¥å·²æœ‰å¿«ç…§æª”æ¡ˆï¼Œå…ˆè®€å‡ºæ¯”å°ï¼Œé¿å…é‡è¤‡å¯«å…¥
    if os.path.exists(snapshot_path):
        try:
            df_existing = pd.read_csv(snapshot_path, encoding=ecode, dtype={'å•†å“ID': str})
            # è½‰å‹å¾Œé€²è¡Œå»é‡ï¼ˆæ ¹æ“šå•†å“ID+æ“·å–æ™‚é–“ï¼‰
            before_len = len(df_snapshot)
            df_combined = pd.concat([df_existing, df_snapshot], ignore_index=True)
            df_combined.drop_duplicates(subset=['å•†å“ID', 'æ“·å–æ™‚é–“'], keep='first', inplace=True)
            new_records = df_combined[~df_combined.duplicated(subset=['å•†å“ID', 'æ“·å–æ™‚é–“'], keep='last')]

            df_snapshot = new_records[df_snapshot.columns]
            actual_new = len(df_snapshot)
            if actual_new == 0:
                print("ğŸš« æ²’æœ‰æ–°å¢çš„å¿«ç…§è³‡æ–™ï¼Œè·³éå¯«å…¥")
            else:
                print(f"âœ… å¯¦éš›å¯«å…¥ {actual_new} ç­†å»é‡å¾Œçš„æ–°å¿«ç…§è³‡æ–™")
                df_snapshot.to_csv(snapshot_path, mode='a', encoding=ecode, index=False, header=False)
        except Exception as e:
            print(f"âŒ è®€å–æˆ–è™•ç†ç¾æœ‰å¿«ç…§æª”æ¡ˆæ™‚å‡ºéŒ¯ï¼š{e}")
    else:
        # ç¬¬ä¸€æ¬¡å»ºç«‹å¿«ç…§æª”
        df_snapshot.to_csv(snapshot_path, encoding=ecode, index=False)
        print(f"âœ… é¦–æ¬¡å»ºç«‹å¿«ç…§æª”ï¼Œå¯«å…¥ {len(df_snapshot)} ç­†")